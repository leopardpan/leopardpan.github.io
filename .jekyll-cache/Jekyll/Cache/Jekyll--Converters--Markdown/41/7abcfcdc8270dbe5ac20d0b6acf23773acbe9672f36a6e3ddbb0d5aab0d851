I"j‘<h1 id="02-æ¦‚ç‡è®ºä¸ä¿¡æ¯è®º">02 æ¦‚ç‡è®ºä¸ä¿¡æ¯è®º</h1>

<h3 id="211-æ¦‚è®ºä¸éšæœºå˜é‡">2.1.1 æ¦‚è®ºä¸éšæœºå˜é‡</h3>
<p>é¢‘ç‡å­¦æ´¾æ¦‚ç‡ï¼ˆfrequency probability):æ¦‚ç‡å’Œäº‹ä»¶å‘ç”Ÿçš„é¢‘ç‡ç›¸å…³ã€‚é¢‘ç‡æ˜¯æŒ‡äº‹ä»¶å‘ç”Ÿçš„æ¬¡æ•°ä¸äº‹ä»¶æ€»æ¬¡æ•°çš„æ¯”å€¼ã€‚é¢‘ç‡å­¦æ´¾æ¦‚ç‡çš„ç¼ºç‚¹æ˜¯ï¼Œå®ƒåªèƒ½ç”¨äºç¦»æ•£äº‹ä»¶ï¼Œè€Œä¸èƒ½ç”¨äºè¿ç»­äº‹ä»¶ã€‚å› ä¸ºè¿ç»­äº‹ä»¶çš„å‘ç”Ÿæ¬¡æ•°æ˜¯æ— ç©·çš„ï¼Œè€Œäº‹ä»¶æ€»æ¬¡æ•°æ˜¯æœ‰é™çš„ï¼Œæ‰€ä»¥å®ƒä»¬çš„æ¯”å€¼æ˜¯æ— ç©·å°çš„ï¼Œè¿™æ ·å°±æ— æ³•ç”¨é¢‘ç‡æ¥è¡¨ç¤ºæ¦‚ç‡äº†ã€‚å› æ­¤ï¼Œé¢‘ç‡å­¦æ´¾æ¦‚ç‡åªèƒ½ç”¨äºç¦»æ•£äº‹ä»¶ï¼Œè€Œä¸èƒ½ç”¨äºè¿ç»­äº‹ä»¶ã€‚
è´å¶æ–¯å­¦æ´¾æ¦‚ç‡ï¼ˆbayesian probabilityï¼‰:æ¦‚ç‡å’Œäº‹ä»¶å‘ç”Ÿçš„å…ˆéªŒçŸ¥è¯†ç›¸å…³ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¦‚ç‡æ˜¯å¯¹æŸä»¶äº‹æƒ…å‘ç”Ÿçš„ä¿¡å¿ƒç¨‹åº¦çš„åº¦é‡ã€‚æ¯æ¬¡äº‹ä»¶å‘ç”Ÿï¼Œéƒ½ä¼šæ›´æ–°å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå¾—åˆ°åéªŒçŸ¥è¯†ã€‚è´å¶æ–¯å­¦æ´¾æ¦‚ç‡çš„ç¼ºç‚¹æ˜¯ï¼Œå®ƒéœ€è¦å…ˆéªŒçŸ¥è¯†ï¼Œè€Œå…ˆéªŒçŸ¥è¯†å¾€å¾€æ˜¯ä¸ç¡®å®šçš„ï¼Œæ‰€ä»¥è´å¶æ–¯å­¦æ´¾æ¦‚ç‡ä¹Ÿæ˜¯ä¸ç¡®å®šçš„ã€‚
éšæœºå˜é‡ï¼ˆrandom variable)å˜é‡ï¼šä¸€ä¸ªå¯èƒ½éšæœºå–ä¸åŒå€¼çš„å˜é‡ï¼Œå¦‚ï¼ŒæŠ›ç¡¬å¸çš„ç»“æœï¼Œæ·éª°å­çš„ç»“æœï¼ŒæŠ½å–çš„æ ·æœ¬çš„ç‰¹å¾ç­‰ã€‚éšæœºå˜é‡çš„å–å€¼æ˜¯éšæœºçš„ï¼Œä½†æ˜¯éšæœºå˜é‡çš„å–å€¼æ˜¯æœ‰é™çš„ï¼Œæ‰€ä»¥éšæœºå˜é‡æ˜¯ç¦»æ•£çš„ã€‚</p>

<h3 id="212-éšæœºå˜é‡çš„åˆ†å¸ƒ">2.1.2 éšæœºå˜é‡çš„åˆ†å¸ƒ</h3>

<p>æ¦‚è®ºè´¨é‡å‡½æ•°(probability Mass Function),å¯¹äºç¦»æ•£å‹å˜é‡ï¼Œæˆ‘ä»¬å…ˆå®šä¹‰ä¸€ä¸ªéšæœºå˜é‡ï¼Œç„¶åç”¨æ¦‚ç‡è´¨é‡å‡½æ•°æ¥æè¿°è¿™ä¸ªéšæœºå˜é‡çš„å–å€¼åˆ†å¸ƒã€‚æ¦‚ç‡è´¨é‡å‡½æ•°æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒçš„è¾“å…¥æ˜¯éšæœºå˜é‡çš„å–å€¼ï¼Œè¾“å‡ºæ˜¯éšæœºå˜é‡å–è¿™ä¸ªå€¼çš„æ¦‚ç‡ã€‚æ¦‚ç‡è´¨é‡å‡½æ•°çš„å®šä¹‰æ˜¯ï¼Œå¯¹äºç¦»æ•£å‹éšæœºå˜é‡Xï¼Œå®ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°æ˜¯ä¸€ä¸ªå‡½æ•°P(x)ï¼Œå®ƒçš„å®šä¹‰åŸŸæ˜¯éšæœºå˜é‡Xçš„å–å€¼é›†åˆã€‚</p>

<p>å¦‚ä¸€ä¸ªç¦»æ•£å‹xæœ‰kä¸ªä¸åŒçš„å–å€¼ï¼Œå‡è®¾xæ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œé‚£ä¹ˆæ¦‚ç‡è´¨é‡å‡½æ•°ä¸ºï¼š<br />
\(P(x=x^i)=\frac{1}{k}\)</p>

<p>æ¦‚è®ºå¯†åº¦å‡½æ•°(probability Density Function),å¯¹äºè¿ç»­å‹å˜é‡ï¼Œæˆ‘ä»¬å…ˆå®šä¹‰ä¸€ä¸ªéšæœºå˜é‡ï¼Œç„¶åç”¨æ¦‚ç‡å¯†åº¦å‡½æ•°æ¥æè¿°è¿™ä¸ªéšæœºå˜é‡çš„å–å€¼åˆ†å¸ƒã€‚æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒçš„è¾“å…¥æ˜¯éšæœºå˜é‡çš„å–å€¼ï¼Œè¾“å‡ºæ˜¯éšæœºå˜é‡å–è¿™ä¸ªå€¼çš„æ¦‚ç‡ã€‚æ¦‚ç‡å¯†åº¦å‡½æ•°çš„å®šä¹‰æ˜¯ï¼Œå¯¹äºè¿ç»­å‹éšæœºå˜é‡Xï¼Œå®ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ä¸€ä¸ªå‡½æ•°P(x)ï¼Œå®ƒçš„å®šä¹‰åŸŸæ˜¯éšæœºå˜é‡Xçš„å–å€¼é›†åˆã€‚Pï¼ˆx)æ˜¯ä¸€ä¸ªéè´Ÿçš„å‡½æ•°ï¼Œå®ƒçš„ç§¯åˆ†æ˜¯1ï¼Œå³ï¼š<br />
\(\int_{-\infty}^{\infty}P(x)dx=1\)
ä¸”æ»¡è¶³éè´Ÿæ€§ï¼š
\(\forall x \in \mathbb{X}, P(x)\geq 0\)</p>

<p>ç´¯è®¡åˆ†å¸ƒå‡½æ•°(cumulative distribution function)è¡¨ç¤ºå¯¹å°äºxçš„æ¦‚ç‡è¿›è¡Œç§¯åˆ†ï¼Œå³ï¼š<br />
\(CDF(x)=\int_{-\infty}^{x}P(x)dx\)</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import uniform
%matplotlib inline
</code></pre>

<pre><code class="language-python"># generate the sample
fig,ax = plt.subplots(1,1)
# ä½¿ç”¨å‚æ•°locå’Œscale,å¯ä»¥å¾—åˆ°[loc, loc + scale]ä¸Šçš„å‡åŒ€åˆ†å¸ƒï¼Œsize=1000è¡¨ç¤ºæ ·æœ¬æ•°ï¼Œrandom_state=123è¡¨ç¤ºéšæœºç§å­
r = uniform.rvs(loc=0,scale=1,size=1000,random_state=123)
# density=Trueè¡¨ç¤ºæ¦‚ç‡å¯†åº¦ï¼Œhisttype='stepfilled'è¡¨ç¤ºå¡«å……ï¼Œalpha=0.5è¡¨ç¤ºé€æ˜åº¦
ax.hist(r,density=True,histtype='stepfilled',alpha=0.5)
# å‡åŒ€åˆ†å¸ƒpdf
x = np.linspace(uniform.ppf(0.01),uniform.ppf(0.99),100)
ax.plot(x,uniform.pdf(x),'r-',lw=5,alpha =0.6,label='uniform pdf')

</code></pre>

<pre><code>[&lt;matplotlib.lines.Line2D at 0x277a8b4d400&gt;]
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_4_1.png" alt="png" /></p>

<h3 id="213-æ¡ä»¶æ¦‚è®ºä¸æ¡ä»¶ç‹¬ç«‹">2.1.3 æ¡ä»¶æ¦‚è®ºä¸æ¡ä»¶ç‹¬ç«‹</h3>

<p>è¾¹ç¼˜æ¦‚è®ºï¼ˆMarginal Probabilityï¼‰å¦‚æœæˆ‘ä»¬çŸ¥é“äº†ä¸€ç»„å˜é‡çš„è”åˆæ¦‚è®ºåˆ†å¸ƒï¼Œæƒ³è¦äº†è§£å…¶ä¸­ä¸€ä¸ªå­é›†çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¿™ç§å®šä¹‰åœ¨å­é›†ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒæˆä¸ºè¾¹ç¼˜æ¦‚è®ºåˆ†å¸ƒ:<br />
\(\forall x \in \mathbb{X},P(\mathrm{x}=x) = \sum_y P(\mathrm{x}=x,\mathrm{y}=y)\)</p>

<p>æ¡ä»¶æ¦‚è®ºï¼ˆconditional probabilityï¼‰åœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸ä»…å…³å¿ƒä¸€ä¸ªäº‹ä»¶çš„æ¦‚ç‡ï¼Œè¿˜å…³å¿ƒåœ¨å¦ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼Œè¿™ä¸ªäº‹ä»¶çš„æ¦‚ç‡ã€‚è¿™ç§åœ¨å¦ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼Œäº‹ä»¶çš„æ¦‚ç‡ç§°ä¸ºæ¡ä»¶æ¦‚è®ºã€‚æ¡ä»¶æ¦‚è®ºçš„å®šä¹‰æ˜¯ï¼š<br />
\(P(\mathrm{x}=x|\mathrm{y}=y)=\frac{P(\mathrm{x}=x,\mathrm{y}=y)}{P(\mathrm{y}=y)}\)
è¿™ä¸ªå…¬å¼è¡¨ç¤ºåœ¨yå‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼Œxå‘ç”Ÿçš„æ¦‚ç‡ã€‚</p>

<p>æ¡ä»¶æ¦‚è®ºçš„é“¾å¼æ³•åˆ™(chain rule of conditional probability)ï¼šä»»ä½•å¤šç»´éšæœºå˜é‡çš„è”åˆæ¦‚è®ºåˆ†å¸ƒï¼Œéƒ½å¯ä»¥åˆ†è§£æˆåªæœ‰åªæœ‰ä¸€ä¸ªå˜é‡çš„æ¡ä»¶æ¦‚è®ºç›¸ä¹˜çš„å½¢å¼<br />
\(P(\mathrm{x}=x,\mathrm{y}=y,\mathrm{z}=z)=P(\mathrm{x}=x|\mathrm{y}=y,\mathrm{z}=z)P(\mathrm{y}=y,\mathrm{z}=z)=P(\mathrm{x}=x|\mathrm{y}=y,\mathrm{z}=z)P(\mathrm{y}=y|\mathrm{z}=z)P(\mathrm{z}=z)\)</p>

\[P(x_1,x_2,\cdots,x_n)=P(x_1|x_2,\cdots,x_n)P(x_2|x_3,\cdots,x_n)\cdots P(x_{n-1}|x_n)P(x_n)\]

<p>ç‹¬ç«‹æ€§ï¼ˆindependenceï¼‰å¦‚æœä¸¤ä¸ªéšæœºå˜é‡Xå’ŒYæ»¡è¶³ï¼š<br />
\(P(\mathrm{x}=x,\mathrm{y}=y)=P(\mathrm{x}=x)P(\mathrm{y}=y)\)
ä¹Ÿå³æ˜¯è”åˆæ¦‚è®ºåˆ†å¸ƒå¯ä»¥è¡¨ç¤ºæˆåªæœ‰ä¸€ä¸ªå˜é‡çš„æ¡ä»¶æ¦‚è®ºç›¸ä¹˜çš„å½¢å¼ã€‚</p>

<p>æ¡ä»¶ç‹¬ç«‹æ€§ï¼ˆconditional independenceï¼‰å¦‚æœå…³äºxå’Œyçš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒå¯¹äºzçš„æ¯ä¸€ä¸ªå€¼éƒ½æ»¡è¶³ç‹¬ç«‹æ€§ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¯´xå’Œyåœ¨zçš„æ¡ä»¶ä¸‹æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ã€‚<br />
\(P(\mathrm{x}=x,\mathrm{y}=y|\mathrm{z}=z)=P(\mathrm{x}=x|\mathrm{z}=z)P(\mathrm{y}=y|\mathrm{z}=z)\)</p>

<h3 id="214-éšæœºå˜é‡çš„åº¦é‡">2.1.4 éšæœºå˜é‡çš„åº¦é‡</h3>
<p>æœŸæœ›ï¼ˆexpectationï¼‰æ˜¯éšæœºå˜é‡çš„å¹³å‡å€¼ï¼ŒæœŸæœ›çš„å®šä¹‰æ˜¯ï¼š<br />
\(E(X)=\sum_x xP(X=x)\)
å¯¹äºè¿ç»­å‹éšæœºå˜é‡å¯ä»¥é€šè¿‡ç§¯åˆ†çš„å½¢å¼è¡¨ç¤ºï¼š<br />
\(E(X)=\int_{-\infty}^{\infty}xP(X=x)dx\)</p>

<p>å¦å¤–ï¼ŒæœŸæœ›æ˜¯çº¿æ€§çš„ï¼Œå³ï¼š<br />
\(E(aX+bY)=aE(X)+bE(Y)\)</p>

<p>æ–¹å·®ï¼ˆvarianceï¼‰æ˜¯éšæœºå˜é‡çš„ç¦»æ•£ç¨‹åº¦ï¼Œæ–¹å·®çš„å®šä¹‰æ˜¯ï¼š<br />
\(Var(X)=E[(X-E(X))^2]\)</p>

<p>æ ‡å‡†å·®ï¼ˆstandard deviationï¼‰æ˜¯æ–¹å·®çš„å¹³æ–¹æ ¹ï¼Œæ ‡å‡†å·®çš„å®šä¹‰æ˜¯ï¼š<br />
\(\sigma(X)=\sqrt{Var(X)}\)</p>

<p>åæ–¹å·®ï¼ˆcovarianceï¼‰æ˜¯ä¸¤ä¸ªéšæœºå˜é‡çš„çº¿æ€§ç›¸å…³ç¨‹åº¦ï¼Œåæ–¹å·®çš„å®šä¹‰æ˜¯ï¼š<br />
\(Cov(X,Y)=E[(X-E(X))(Y-E(Y))]\)</p>

<p><strong>æ³¨æ„</strong>åæ–¹å·®çš„ç¬¦å·è¡¨ç¤ºä¸¤ä¸ªéšæœºå˜é‡çš„çº¿æ€§ç›¸å…³ç¨‹åº¦ï¼Œæ­£æ•°è¡¨ç¤ºæ­£ç›¸å…³ï¼Œè´Ÿæ•°è¡¨ç¤ºè´Ÿç›¸å…³ï¼Œ0è¡¨ç¤ºä¸ç›¸å…³ã€‚<br />
<strong>æ³¨æ„</strong>ç‹¬ç«‹ä¸é›¶åæ–¹å·®æ›´å¼ºï¼Œå› ä¸ºç‹¬ç«‹è¿˜æ’é™¤äº†éçº¿æ€§çš„ç›¸å…³ã€‚</p>

<pre><code class="language-python">x= np.array([1,2,3,4,5,6,7,8,9,10])
y = np.array([10,9,8,7,6,5,4,3,2,1])
Mean = np.mean(x)
Var = np.var(x)
Var_unbiased = np.var(x,ddof=1)# ddof=1è¡¨ç¤ºæ— åä¼°è®¡
Std = np.std(x)
Cov = np.cov(x,y)
Mean,Var,Var_unbiased,Std,Cov
</code></pre>

<pre><code>(5.5,
 8.25,
 9.166666666666666,
 2.8722813232690143,
 array([[ 9.16666667, -9.16666667],
        [-9.16666667,  9.16666667]]))
</code></pre>

<h3 id="215-å¸¸ç”¨æ¦‚ç‡åˆ†å¸ƒ">2.1.5 å¸¸ç”¨æ¦‚ç‡åˆ†å¸ƒ</h3>
<p>ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆBernoulli distributionï¼‰æ˜¯ä¸€ä¸ªäºŒé¡¹åˆ†å¸ƒçš„ç‰¹ä¾‹ï¼Œå®ƒåªæœ‰ä¸¤ä¸ªå¯èƒ½çš„å–å€¼ï¼Œ0å’Œ1ï¼Œå®ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°æ˜¯ï¼ˆè¡¨ç¤ºä¸€æ¬¡å®éªŒæˆåŠŸçš„æ¦‚ç‡ï¼‰ï¼š<br />
\(P(X=1)=p\)
\(P(X=0)=1-p\)
\(P(X=x)=p^x(1-p)^{1-x}\)</p>

<pre><code class="language-python">def plot_distribution(X,axes=None):
    '''ç»™å®šéšæœºå˜é‡Xï¼Œç»˜åˆ¶å…¶PDF,PMF,CDF'''
    if axes is None:
        fig,axes = plt.subplots(1,3,figsize=(12,4))
    x_min,x_max = X.interval(0.99)# 99%çš„æ¦‚ç‡å¯†åº¦åœ¨åŒºé—´å†…
    x = np.linspace(x_min,x_max,1000)
    # è¿ç»­å‹å˜é‡ï¼Œç”»PDFï¼›ç¦»æ•£å‹å˜é‡ï¼Œç”»PMF
    if hasattr(X.dist,'pdf'):
        # axes[0]è¡¨ç¤ºç¬¬ä¸€è¡Œï¼Œç¬¬ä¸€åˆ—
        axes[0].plot(x,X.pdf(x),label='PDF')
        axes[0].fill_between(x,X.pdf(x),alpha=0.5) # å¡«å……,é€æ˜åº¦0.5
    else:
        x_int = np.unique(x.astype(int))# å–æ•´,uniqueå»é‡
        axes[0].bar(x_int,X.pmf(x_int),label='PMF') 
    # CDF
    axes[1].plot(x,X.cdf(x),label='CDF')
    for ax in axes:
        ax.legend()
    return axes
</code></pre>

<pre><code class="language-python">from scipy.stats import bernoulli
fig,axes = plt.subplots(1,2,figsize=(12,4))
r = bernoulli(p=0.6) # ç”Ÿæˆä¼¯åŠªåˆ©åˆ†å¸ƒ
plot_distribution(r,axes)
</code></pre>

<pre><code>array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000277A9C8B8E0&gt;,
       &lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000277A9CB3100&gt;],
      dtype=object)
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_10_1.png" alt="png" /></p>

<pre><code class="language-python"># ç”Ÿæˆå®éªŒæˆåŠŸçš„æ¬¡æ•°
p = 0.6
def trials(n_samples):
    # äºŒé¡¹åˆ†å¸ƒï¼Œn_samplesè¡¨ç¤ºå®éªŒæ¬¡æ•°ï¼Œpè¡¨ç¤ºæˆåŠŸçš„æ¦‚ç‡ï¼Œsamplesè¡¨ç¤ºæˆåŠŸçš„æ¬¡æ•°
    samples = np.random.binomial(n_samples,p) 
    proba_zero = (n_samples-samples)/n_samples# 0æ¬¡æˆåŠŸçš„æ¦‚ç‡
    proba_one = samples/n_samples# 1æ¬¡æˆåŠŸçš„æ¦‚ç‡
    return [proba_zero,proba_one]
fig,axes = plt.subplots(1,2,figsize=(12,4))
# ä¸€æ¬¡å®éªŒï¼Œä¼¯åŠªåˆ©åˆ†å¸ƒ
n_samples = 1
axes[0].bar([0,1],trials(n_samples),label ='Bernoulli')
axes[0].set_title('n_samples = {}'.format(n_samples))
# næ¬¡å®éªŒï¼ŒäºŒé¡¹åˆ†å¸ƒ
n_samples = 1000
axes[1].bar([0,1],trials(n_samples),label ='Binomial')
axes[1].set_title('n_samples = {}'.format(n_samples))
for ax in axes:
    ax.legend()
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_11_0.png" alt="png" /></p>

<p>èŒƒç•´åˆ†å¸ƒï¼ˆCategorical distributionï¼Œåˆ†ç±»åˆ†å¸ƒï¼‰æ˜¯ä¸€ä¸ªå¤šé¡¹åˆ†å¸ƒçš„ç‰¹ä¾‹ï¼Œå®ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°æ˜¯ï¼ˆè¡¨ç¤ºç¬¬kç±»çš„æ¦‚ç‡ï¼‰ï¼š<br />
\(P(X=k)=p_k\)</p>

<p>ä¾‹å¦‚æ¯æ¬¡è¯•éªŒçš„ç»“æœå°±å¯ä»¥è¡¨ç¤ºæˆä¸€ä¸ªkç»´çš„å‘é‡ï¼Œæ¯ä¸ªç»´åº¦è¡¨ç¤ºä¸€ç§ç»“æœï¼Œåªæœ‰æ­¤æ¬¡è¯•éªŒçš„ç»“æœå¯¹åº”çš„ç»´åº¦ä¸º1ï¼Œå…¶ä½™ç»´åº¦ä¸º0ï¼Œä¾‹å¦‚ï¼š<br />
\(P(X=(1,0,0,0))=p_1\) <br />
\(P(X=(0,1,0,0))=p_2\) <br />
\(P(X=(0,0,1,0))=p_3\) <br />
\(P(X=(0,0,0,1))=p_4\)</p>

<pre><code class="language-python">def k_possibilities(k):
    '''ç”Ÿæˆkä¸ªå¯èƒ½çš„ç»“æœ'''
    res = np.random.rand(k)# random.rand(k)ç”Ÿæˆkä¸ª[0,1)ä¹‹é—´çš„éšæœºæ•°
    _sum = sum(res) # æ±‚å’Œ
    for i,x in enumerate(res):
        res[i] = x/_sum # å½’ä¸€åŒ–,enumerateè¿”å›ç´¢å¼•å’Œå€¼,iè¡¨ç¤ºç´¢å¼•ï¼Œxè¡¨ç¤ºå€¼
    return res
fig,axes = plt.subplots(1,2,figsize=(12,4))
# ä¸€æ¬¡å®éªŒï¼ŒèŒƒç•´åˆ†å¸ƒ
n_samples = 1
k=10
samples = np.random.multinomial(n_samples,k_possibilities(k))
axes[0].bar(range(len(samples)),samples/n_samples,label ='Multinomial')

# næ¬¡å®éªŒï¼Œå¤šé¡¹åˆ†å¸ƒ
n_samples = 1000
samples = np.random.multinomial(n_samples,k_possibilities(k))
axes[1].bar(range(len(samples)),samples/n_samples,label ='Multinomial')
for ax in axes:
    ax.legend()
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_13_0.png" alt="png" /></p>

<p>é«˜æ–¯åˆ†å¸ƒï¼ˆGaussian distributionï¼Œæ­£æ€åˆ†å¸ƒï¼‰æ˜¯ä¸€ä¸ªè¿ç»­å‹æ¦‚ç‡åˆ†å¸ƒï¼Œè‹¥éšæœºå˜é‡$X\sim \mathcal{N}(\mu,\sigma^2)$,å®ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ï¼š<br />
\(P(X=x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\) <br />
æœ‰æ—¶ä¹Ÿä¼šç”¨$\beta = \frac{1}{\sigma^2}$æ¥è¡¨ç¤ºåˆ†å¸ƒçš„ç²¾åº¦(ç²¾åº¦è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šç‹­çª„ï¼Œprecision)ï¼š<br />
\(P(X=x)=\frac{1}{\sqrt{2\pi\beta^{-1}}}e^{-\frac{(x-\mu)^2}{2\beta}}\)</p>

<p>ä¸­å¿ƒæé™å®šç†ï¼ˆcentral limit theoremï¼‰æ˜¯ä¸€ä¸ªé‡è¦çš„å®šç†ï¼Œå®ƒè¡¨ç¤ºå½“æ ·æœ¬é‡è¶³å¤Ÿå¤§æ—¶ï¼Œæ ·æœ¬å‡å€¼çš„åˆ†å¸ƒä¼šè¶‹è¿‘äºæ­£æ€åˆ†å¸ƒï¼Œå¦‚å™ªå£°çš„åˆ†å¸ƒå°±æ˜¯ä¸€ä¸ªå…¸å‹çš„ä¾‹å­ï¼Œå™ªå£°çš„åˆ†å¸ƒæ˜¯å‡åŒ€åˆ†å¸ƒï¼Œä½†æ˜¯å½“å™ªå£°çš„æ•°é‡è¶³å¤Ÿå¤šæ—¶ï¼Œå™ªå£°çš„å‡å€¼çš„åˆ†å¸ƒå°±ä¼šè¶‹è¿‘äºæ­£æ€åˆ†å¸ƒã€‚</p>

<p>å¦‚æœæˆ‘ä»¬å¯¹éšæœºå˜é‡è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå³å°†éšæœºå˜é‡å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®ï¼Œé‚£ä¹ˆæ ‡å‡†åŒ–åçš„éšæœºå˜é‡çš„å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼Œè¿™æ ·çš„éšæœºå˜é‡ç§°ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆstandard normal distributionï¼‰ï¼Œ<br />
\(Z=\frac{X-\mu}{\sigma}\)</p>

<pre><code class="language-python">from scipy.stats import norm
fig,axes = plt.subplots(1,2,figsize= (12,4))
mu,sigma = 0,1# å‡å€¼ï¼Œæ ‡å‡†å·®
r = norm(loc=mu,scale=sigma)# ç”Ÿæˆæ­£æ€åˆ†å¸ƒ
plot_distribution(r,axes) #ç»˜åˆ¶rçš„PDFå’ŒCDF
</code></pre>

<pre><code>array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001B387029910&gt;,
       &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001B386E8E430&gt;],
      dtype=object)
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_15_1.png" alt="png" /></p>

<p>å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼ˆmultivariate Gaussian distributionï¼‰æ˜¯ä¸€ä¸ªå¤šç»´çš„é«˜æ–¯åˆ†å¸ƒï¼Œå®ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ï¼š<br />
\(P(X=x;\sigma,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}\)</p>

<p>å…·ä½“æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ï¼š<br />
å…ˆå‡è®¾ $\mathrm{n}$ ä¸ªå˜é‡ $x=\left[x_1, x_2, \cdots, x_n\right]^{\mathrm{T}}$ äº’ä¸ç›¸å…³ï¼Œä¸”æœä»æ­£æ€åˆ†å¸ƒ (ç»´åº¦ä¸ç›¸å…³å¤šå…ƒæ­£æ€åˆ†å¸ƒ)ï¼Œå„ä¸ªç»´åº¦çš„å‡å€¼ $E(x)=\left[\mu_1, \mu_2, \cdots, \mu_n\right]^{\mathrm{T}}$ ï¼Œæ–¹å·® $\sigma(x)=\left[\sigma_1, \sigma_2, \cdots, \sigma_n\right]^{\mathrm{T}}$
æ ¹æ®è”åˆæ¦‚ç‡å¯†åº¦å…¬å¼:
\(\begin{aligned}
&amp; f(x)=p\left(x_1, x_2 \ldots x_n\right)=p\left(x_1\right) p\left(x_2\right) \ldots p\left(x_n\right)=\frac{1}{(\sqrt{2 \pi})^n \sigma_1 \sigma_2 \cdots \sigma_n} e^{-\frac{\left(x_1-\mu_1\right)^2}{2 \sigma_1^2}-\frac{\left(x_2-\mu_2\right)^2}{2 \sigma_2^2} \cdots-\frac{\left(x_n-\mu_n\right)^2}{2 \sigma_n}} \\
&amp; \text { ä»¤ } z^2=\frac{\left(x_1-\mu_1\right)^2}{\sigma_1^2}+\frac{\left(x_2-\mu_2\right)^2}{\sigma_2^2} \cdots+\frac{\left(x_n-\mu_n\right)^2}{\sigma_n^2}, \sigma_z=\sigma_1 \sigma_2 \cdots \sigma_n
\end{aligned}\)
è¿™æ ·å¤šå…ƒæ­£æ€åˆ†å¸ƒåˆå¯ä»¥å†™æˆä¸€å…ƒé‚£ç§æ¼‚äº®çš„å½¢å¼äº†(æ³¨æ„ä¸€å…ƒä¸å¤šå…ƒçš„å·®åˆ«):
\(f(z)=\frac{1}{(\sqrt{2 \pi})^n \sigma_z} e^{-\frac{z^2}{2}}\)
å› ä¸ºå¤šå…ƒæ­£æ€åˆ†å¸ƒæœ‰ç€å¾ˆå¼ºçš„å‡ ä½•æ€æƒ³ï¼Œå•çº¯ä»ä»£æ•°çš„è§’åº¦çœ‹å¾…zå¾ˆéš¾çœ‹å‡º zçš„æ¦‚ç‡åˆ†å¸ƒè§„å¾‹ï¼Œè¿™é‡Œéœ€è¦è½¬æ¢æˆçŸ©é˜µå½¢å¼:
\(z^2=z^{\mathrm{T}} z=\left[x_1-\mu_1, x_2-\mu_2, \cdots, x_n-\mu_n\right]\left[\begin{array}{cccc}
\frac{1}{\sigma_1^2} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \frac{1}{\sigma_2^2} &amp; \cdots &amp; 0 \\
\vdots &amp; \cdots &amp; \cdots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \frac{1}{\sigma_n^2}
\end{array}\right]\left[x_1-\mu_1, x_2-\mu_2, \cdots, x_n-\mu_n\right]^{\mathrm{T}}\)
ç­‰å¼æ¯”è¾ƒé•¿ï¼Œè®©æˆ‘ä»¬è¦åšä¸€ä¸‹å˜é‡æ›¿æ¢:
\(x-\mu_x=\left[x_1-\mu_1, x_2-\mu_2, \cdots, x_n-\mu_n\right]^{\mathrm{T}}\)</p>

<p>å®šä¹‰ä¸€ä¸ªç¬¦å·
\(\sum=\left[\begin{array}{cccc}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \cdots &amp; \cdots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma_n^2
\end{array}\right]\)
$\sum$ ä»£è¡¨å˜é‡ $\mathrm{X}$ çš„åæ–¹å·®çŸ©é˜µï¼Œ iè¡Œjåˆ—çš„å…ƒç´ å€¼è¡¨ç¤º $x_i$ ä¸ $x_j$ çš„åæ–¹å·®
å› ä¸ºç°åœ¨å˜é‡ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œæ‰€ä»¥åªæœ‰å¯¹è§’çº¿ä¸Š $(i=j)$ å­˜åœ¨å…ƒç´ ï¼Œå…¶ä»–åœ°æ–¹éƒ½ç­‰äº 0 ï¼Œä¸” $x_i$ ä¸å®ƒæœ¬èº«çš„åæ–¹å·®å°±ç­‰äºæ–¹å·®ã€‚<br />
$\sum$ æ˜¯ä¸€ä¸ªå¯¹è§’é˜µï¼Œæ ¹æ®å¯¹è§’çŸ©é˜µçš„æ€§è´¨ï¼Œå®ƒçš„é€†çŸ©é˜µ:
\(\left(\sum\right)^{-1}=\left[\begin{array}{cccc}
\frac{1}{\sigma_1^2} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \frac{1}{\sigma_2^2} &amp; \cdots &amp; 0 \\
\vdots &amp; \cdots &amp; \cdots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \frac{1}{\sigma_n^2}
\end{array}\right].\)
å¯¹è§’çŸ©é˜µçš„è¡Œåˆ—å¼ $=$ å¯¹è§’å…ƒç´ çš„ä¹˜ç§¯
\(\sigma_z=\left|\sum\right|^{\frac{1}{2}}=\sigma_1 \sigma_2 \ldots \sigma_n\)
æ›¿æ¢å˜é‡ä¹‹åï¼Œç­‰å¼å¯ä»¥ç®€åŒ–ä¸º:
\(z^{\mathrm{T}} z=\left(x-\mu_x\right)^{\mathrm{T}} \sum^{-1}\left(x-\mu_x\right)\)
ä»£å…¥ä»¥zä¸ºè‡ªå˜é‡çš„æ ‡å‡†é«˜æ–¯åˆ†å¸ƒå‡½æ•°ä¸­:
\(f(z)=\frac{1}{(\sqrt{2 \pi})^n \sigma_z} e^{-\frac{z^2}{2}}=\frac{1}{(\sqrt{2 \pi})^n \sum^{\frac{1}{2}}} e^{-\frac{\left(x-\mu_x\right)^T(\Gamma)^{-1}\left(x-\mu_x\right)}{2}}\)
æ³¨æ„å‰é¢çš„ç³»æ•°å˜åŒ–: ä»éæ ‡å‡†æ­£æ€åˆ†å¸ƒ $-&gt;$ æ ‡å‡†æ­£æ€åˆ†å¸ƒéœ€è¦å°†æ¦‚ç‡å¯†åº¦å‡½æ•°çš„é«˜åº¦å‹ç¼© $\left|\sum\right|^{\frac{1}{2}}$ å€ï¼Œä»ä¸€ç»´ $\rightarrow$ nç»´çš„è¿‡ ç¨‹ä¸­ï¼Œæ¯å¢åŠ ä¸€ç»´ï¼Œé«˜åº¦å°†å‹ç¼© $\sqrt{2 \pi}$ å€</p>

<p>reference: 
https://www.cnblogs.com/bingjianing/p/9117330.html</p>

<pre><code class="language-python">from scipy.stats import multivariate_normal
x,y = np.mgrid[-1:1:.01,-1:1:.01]# ç”Ÿæˆç½‘æ ¼ç‚¹,æ­¥é•¿ä¸º0.01
pos = np.dstack((x,y))# ç”Ÿæˆç½‘æ ¼ç‚¹åæ ‡,shape=(200,200,2)
sigma = [[2,0.3],[0.3,0.5]] # åæ–¹å·®çŸ©é˜µ
mu = [0.5,0.2]
X = multivariate_normal(mu,sigma)# ç”Ÿæˆå¤šå…ƒæ­£æ€åˆ†å¸ƒ
fig,axes = plt.subplots(1,1,figsize=(12,4)) 
axes.contourf(x,y,X.pdf(pos))# ç»˜åˆ¶ç­‰é«˜çº¿,posè¡¨ç¤ºåœ¨ç½‘æ ¼ç‚¹ä¸Šçš„æ¦‚ç‡å¯†åº¦
</code></pre>

<pre><code>&lt;matplotlib.contour.QuadContourSet at 0x1b386e185b0&gt;
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_17_1.png" alt="png" /></p>

<p>æŒ‡æ•°åˆ†å¸ƒï¼ˆExponential distributionï¼‰æ˜¯æè¿°æ³Šæ¾è¿‡ç¨‹ä¸­çš„äº‹ä»¶ä¹‹é—´çš„æ—¶é—´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå³äº‹ä»¶ä»¥æ’å®šå¹³å‡é€Ÿç‡è¿ç»­ä¸”ç‹¬ç«‹åœ°å‘ç”Ÿçš„è¿‡ç¨‹ã€‚ è¿™æ˜¯ä¼½é©¬åˆ†å¸ƒçš„ä¸€ä¸ªç‰¹æ®Šæƒ…å†µã€‚ å®ƒæ˜¯å‡ ä½•åˆ†å¸ƒçš„è¿ç»­æ¨¡æ‹Ÿï¼Œå®ƒå…·æœ‰æ— è®°å¿†ï¼ˆMemoryless Property)çš„å…³é”®æ€§è´¨,ä¹Ÿå³æ˜¯$P(T&gt;t+s|T&gt;t)=P(T&gt;s)$ã€‚ é™¤äº†ç”¨äºåˆ†ææ³Šæ¾è¿‡ç¨‹å¤–ï¼Œè¿˜å¯ä»¥åœ¨å…¶ä»–å„ç§ç¯å¢ƒä¸­æ‰¾åˆ°ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š<br />
\(p(x)=\left\{\begin{array}{c}
\frac{1}{\theta} e^{-\frac{x}{\theta}}, x&gt;0 \\
0, x \leq 0
\end{array} \quad(\theta&gt;0)\right.\)
åˆ™ç§° éšæœºå˜é‡$X$ æœä»å‚æ•°ä¸º $\theta$ çš„æŒ‡æ•°åˆ†å¸ƒ, è®°ä¸º $X \sim \operatorname{EXP}(\theta)$.
å…¶ä¸­ $\lambda = \frac{1}{\theta}$ ç§°ä¸ºç‡å‚æ•°ï¼ˆrate parameterï¼‰ã€‚å³æ¯å•ä½æ—¶é—´å†…å‘ç”ŸæŸäº‹ä»¶çš„æ¬¡æ•°ï¼ˆæ¯”æ–¹è¯´ï¼šå¦‚æœä½ å¹³å‡æ¯ä¸ªå°æ—¶æ¥åˆ°2æ¬¡ç”µè¯ï¼Œé‚£ä¹ˆä½ é¢„æœŸç­‰å¾…æ¯ä¸€æ¬¡ç”µè¯çš„æ—¶é—´æ˜¯åŠä¸ªå°æ—¶ï¼‰ã€‚æŒ‡æ•°åˆ†å¸ƒçš„æœŸæœ›å’Œæ–¹å·®åˆ†åˆ«ä¸º $\theta$ å’Œ $\theta^2$.</p>

<p>æŒ‡æ•°åˆ†å¸ƒçš„ç´¯è®¡åˆ†å¸ƒå‡½æ•°ä¸ºï¼š<br />
\(P(X \leq x) = 1 - e^{-\frac{x}{\theta}}\)</p>

<pre><code class="language-python">from scipy.stats import expon
fig,axes = plt.subplots(1,2,figsize=(12,4))
r = expon(scale=2)# ç”ŸæˆæŒ‡æ•°åˆ†å¸ƒï¼Œscale=2è¡¨ç¤ºÎ»=1/2
plot_distribution(r,axes)

</code></pre>

<pre><code>array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001B387102DF0&gt;,
       &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001B387165340&gt;],
      dtype=object)
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_19_1.png" alt="png" /></p>

<p>æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒï¼ˆLaplace distributionï¼‰æ˜¯ä¸€ç§è¿ç»­æ¦‚ç‡åˆ†å¸ƒï¼Œå®ƒæ˜¯åŒæŒ‡æ•°åˆ†å¸ƒçš„ç‰¹æ®Šæƒ…å†µã€‚å®ƒæ˜¯ä¸€ç§å¯¹ç§°åˆ†å¸ƒï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š<br />
\(p(x) = \frac{1}{2 \lambda} e^{ - \frac{|x - \mu|}{\lambda}}\)</p>

<p>å…¶ä¸­ $\mu$ ç§°ä¸ºåˆ†å¸ƒçš„ä½ç½®å‚æ•°ï¼ˆlocation parameterï¼‰ï¼Œ$\lambda$ ç§°ä¸ºåˆ†å¸ƒçš„å°ºåº¦å‚æ•°ï¼ˆscale parameterï¼‰ã€‚æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒçš„æœŸæœ›å’Œæ–¹å·®åˆ†åˆ«ä¸º $\mu$ å’Œ $2 \lambda^2$.</p>

<pre><code class="language-python">from scipy.stats import laplace
fig,axes = plt.subplots(1,2,figsize=(12,4))
mu,gamma = 0,1 # å‡å€¼ï¼Œæ ‡å‡†å·®
r = laplace(loc=0,scale=1)# ç”Ÿæˆæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ
plot_distribution(r,axes)
</code></pre>

<pre><code>array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001B3870DF130&gt;,
       &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001B385D4DE20&gt;],
      dtype=object)
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_21_1.png" alt="png" /></p>

<p>Dirac åˆ†å¸ƒ
Dirac delta functionï¼Œä¹Ÿç§°ä¸º Dirac åˆ†å¸ƒã€‚</p>

<p>Dirichletåˆ†å¸ƒ</p>

<p>åŸºç¡€çŸ¥è¯†ï¼šconjugate priorså…±è½­å…ˆéªŒ
å…±è½­å…ˆéªŒæ˜¯æŒ‡è¿™æ ·ä¸€ç§æ¦‚ç‡å¯†åº¦ï¼šå®ƒä½¿å¾—åéªŒæ¦‚ç‡çš„å¯†åº¦å‡½æ•°ä¸å…ˆéªŒæ¦‚ç‡çš„å¯†åº¦å‡½æ•°å…·æœ‰ç›¸åŒçš„å‡½æ•°å½¢å¼ã€‚å®ƒæå¤§åœ°ç®€åŒ–äº†è´å¶æ–¯åˆ†æã€‚
å¦‚ä½•è§£é‡Šè¿™å¥è¯ã€‚ç”±äº
P(u|D) = p(D|u)p(u)/p(D) <br />
å…¶ä¸­Dæ˜¯ç»™å®šçš„ä¸€ä¸ªæ ·æœ¬é›†åˆï¼Œå› æ­¤å¯¹å…¶æ¥è¯´p(D)æ˜¯ä¸€ä¸ªç¡®å®šçš„å€¼ï¼Œå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªå¸¸æ•°ã€‚P(u|D)æ˜¯åéªŒæ¦‚ç‡â€”-å³è§‚å¯Ÿåˆ°ä¸€ç³»åˆ—æ ·æœ¬æ•°æ®åæ¨¡å‹å‚æ•°æœä»çš„æ¦‚ç‡ï¼Œp(D|u)æ˜¯ä¼¼ç„¶æ¦‚ç‡â€”-åœ¨ç»™å®šçš„æ¨¡å‹å‚æ•°uä¸‹æ ·æœ¬æ•°æ®æœä»è¿™ä¸€æ¦‚ç‡æ¨¡å‹çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œp(u)æ˜¯uçš„å…ˆéªŒæ¦‚ç‡â€”-åœ¨æˆ‘ä»¬ä¸€æ— æ‰€çŸ¥çš„æƒ…å†µä¸‹uçš„æ¦‚ç‡åˆ†å¸ƒã€‚P(u|D)çš„å‡½æ•°å½¢å¼å®Œå…¨ç”±p(D|u)å’Œp(u)çš„ä¹˜ç§¯å†³å®šã€‚å¦‚æœp(u)çš„å–å€¼ä½¿p(u|D)å’Œp(D|u)ç›¸åŒçš„è¡¨è¾¾å½¢å¼ï¼ˆå…³äºuçš„è¡¨è¾¾å½¢å¼ï¼‰ï¼Œå°±ç§°p(u)ä¸ºå…±è½­å…ˆéªŒã€‚</p>

<p>reference: 
<br />
[1]<a href="https://blog.csdn.net/jwh_bupt/article/details/8841644">æœºå™¨å­¦ä¹ çš„æ•°å­¦åŸºç¡€ï¼ˆ1ï¼‰â€“Dirichletåˆ†å¸ƒ</a></p>

<p>Betaåˆ†å¸ƒå¼äºŒé¡¹åˆ†å¸ƒçš„å…±è½­å…ˆéªŒåˆ†å¸ƒ.</p>

<p>Dirichletåˆ†å¸ƒæ˜¯ä¸€ä¸ªå¤šç»´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå®ƒæ˜¯ä¸€ç»„(å¤šå…ƒï¼‰ç‹¬ç«‹çš„ $\operatorname{Beta}$ åˆ†å¸ƒçš„å…±è½­å…ˆéªŒåˆ†å¸ƒã€‚åœ¨è´å¶æ–¯æ¨æ–­ï¼ˆBayesian inferenceï¼‰ä¸­ï¼Œç‹„åˆ©å…‹é›·åˆ†å¸ƒä½œä¸ºå¤šé¡¹åˆ†å¸ƒçš„å…±è½­å…ˆéªŒå¾—åˆ°åº”ç”¨ [3]  ï¼Œåœ¨æœºå™¨å­¦ä¹ ï¼ˆmachine learningï¼‰ä¸­è¢«ç”¨äºæ„å»ºç‹„åˆ©å…‹é›·æ··åˆæ¨¡å‹ï¼ˆDirichlet mixture modelï¼‰
æ˜¯ä¸€ç±»åœ¨ å®æ•°åŸŸ ä»¥æ­£å•çº¯å½¢ï¼ˆstandard simplexï¼‰ä¸º æ”¯æ’‘é›† ï¼ˆsupportï¼‰çš„é«˜ç»´ è¿ç»­æ¦‚ç‡åˆ†å¸ƒ ï¼Œæ˜¯Betaåˆ†å¸ƒåœ¨é«˜ç»´æƒ…å½¢çš„æ¨å¹¿ã€‚</p>

<p>reference: <br />
[1]<a href="https://zhuanlan.zhihu.com/p/37976562">ç†è§£Gammaåˆ†å¸ƒã€Betaåˆ†å¸ƒä¸Dirichletåˆ†å¸ƒ
</a>
<br />
[2]<a href="https://zhuanlan.zhihu.com/p/76991275">æœ€é€šä¿—æ˜“æ‡‚çš„ç™½è¯ç‹„åˆ©å…‹é›·è¿‡ç¨‹(Dirichlet Processï¼‰</a></p>

<h3 id="216-å¸¸è§å‡½æ•°çš„æœ‰ç”¨æ€§è´¨">2.1.6 å¸¸è§å‡½æ•°çš„æœ‰ç”¨æ€§è´¨</h3>

<p>logisitc sigmoidå‡½æ•°é€šå¸¸ç”¨æ¥äº§ç”Ÿä¼¯åŠªåˆ©åˆ†å¸ƒçš„æ¦‚ç‡ï¼Œå› ä¸ºå®ƒçš„èŒƒå›´æ˜¯ (0, 1)ï¼Œå› æ­¤å®ƒçš„å¯¼æ•°æ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚sigmoid å‡½æ•°åœ¨å˜é‡å–ç»å¯¹å€¼â¾®å¸¸â¼¤çš„æ­£å€¼æˆ–è´Ÿå€¼æ—¶ä¼šå‡ºç°é¥±å’Œ (Saturate)ç°è±¡ï¼Œæ„å‘³ç€å‡½æ•°ä¼šå˜å¾—å¾ˆå¹³ï¼Œå¹¶ä¸”å¯¹è¾“â¼Šçš„å¾®â¼©æ”¹å˜ä¼šå˜å¾—ä¸æ•æ„Ÿã€‚<br />
\(sigmoid(x) = \frac{1}{1+e^{-x}}\)</p>

<p>softpluså‡½æ•°
\(softplus(x) = \log(1+e^x)\)</p>

<p>å› ä¸ºå®ƒçš„èŒƒå›´æ˜¯ (0, âˆ)ï¼Œsoftpluså‡½æ•°å¯ä»¥ç”¨æ¥äº§ç”Ÿæ­£å¤ªåˆ†å¸ƒçš„$\beta$å’Œ$\sigma$å‚æ•°ï¼Œå› æ­¤å®ƒçš„å¯¼æ•°æ˜¯æ­£å¤ªåˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚
softpluså‡½æ•°åç§°æ¥æºäºå®ƒæ˜¯ä¸€ä¸ªå¹³æ»‘çš„ReLUå‡½æ•°ï¼ŒReLUå‡½æ•°æ˜¯ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œå®ƒçš„å®šä¹‰æ˜¯
\(ReLU(x) = max(0, x)\)</p>

<pre><code class="language-python">x = np.linspace(-10,10,1000)
sigmoid = 1/(1+np.exp(-x))
softplus = np.log(1+np.exp(x))
fig,axes = plt.subplots(1,2,figsize=(12,4))
axes[0].plot(x,sigmoid)
axes[0].set_title('Sigmoid')
axes[1].plot(x,softplus)
axes[1].set_title('Softplus')

</code></pre>

<pre><code>Text(0.5, 1.0, 'Softplus')
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_26_1.png" alt="png" /></p>

<h2 id="22-ä¿¡æ¯è®º">2.2 ä¿¡æ¯è®º</h2>
<p>ä¿¡æ¯è®ºèƒŒåçš„æ€æƒ³ï¼šä¸€ä»¶ä¸å¤ªå¯èƒ½çš„äº‹ä»¶æ¯”ä¸€ä»¶æ¯”è¾ƒå¯èƒ½çš„äº‹ä»¶æ›´æœ‰ä¿¡æ¯é‡ã€‚ï¼ˆä¿¡æ¯é‡çš„åº¦é‡å°±ç­‰äºä¸ç¡®å®šæ€§çš„å¤šå°‘ã€‚ï¼‰<br />
ä¿¡æ¯ (Information) éœ€è¦æ»¡â¾œçš„ä¸‰ä¸ªæ¡ä»¶ï¼š</p>
<ul>
  <li>â½è¾ƒå¯èƒ½å‘â½£çš„äº‹ä»¶çš„ä¿¡æ¯é‡è¦å°‘</li>
  <li>â½è¾ƒä¸å¯èƒ½å‘â½£çš„äº‹ä»¶çš„ä¿¡æ¯é‡è¦â¼¤</li>
  <li>ç‹¬â½´å‘â½£çš„äº‹ä»¶ä¹‹é—´çš„ä¿¡æ¯é‡åº”è¯¥æ˜¯å¯ä»¥å åŠ çš„ã€‚ä¾‹å¦‚ï¼ŒæŠ•æ·çš„ç¡¬å¸ä¸¤æ¬¡æ­£â¾¯æœä¸Šä¼ é€’çš„ä¿¡æ¯é‡ï¼Œåº”è¯¥æ˜¯æŠ•æ·â¼€æ¬¡ç¡¬å¸æ­£â¾¯æœä¸Šçš„ä¿¡æ¯é‡çš„ä¸¤å€</li>
</ul>

<p>è‡ªä¿¡æ¯ (Self-information) æ˜¯ä¿¡æ¯è®ºä¸­çš„åŸºæœ¬æ¦‚å¿µï¼Œå®ƒæ˜¯æŒ‡å•ä¸ªäº‹ä»¶å‘â½£çš„ä¿¡æ¯é‡ã€‚è‡ªä¿¡æ¯çš„å®šä¹‰æ˜¯<br />
\(I(x) = -\log P(x)\)</p>

<p>é¦™å†œç†µ (Shannon Entropy)æ˜¯å¯¹äºæ•´ä¸ªæ¦‚è®ºåˆ†å¸ƒPï¼Œéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§çš„è¡¡é‡ï¼š<br />
\(H(P) = -ï¼ˆ p_1 * \log p_1 +  p_2 * \log p_2 + \cdots +  p_n * \log p_n) =-\sum_{x}P(x)\log P(x)\)</p>

<p>è”åˆç†µ (Joint Entropy) æ˜¯å¯¹äºä¸¤ä¸ªéšæœºå˜é‡Xå’ŒYï¼Œå®ƒä»¬çš„è”åˆåˆ†å¸ƒP(X, Y)ï¼Œéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§çš„è¡¡é‡ï¼š<br />
\(H(X, Y) = -\sum_{x,y}P(x, y)\log P(x, y)\)</p>

<p>æ¡ä»¶ç†µ (Conditional Entropy) æ˜¯å¯¹äºä¸¤ä¸ªéšæœºå˜é‡Xå’ŒYï¼Œå®ƒä»¬çš„è”åˆåˆ†å¸ƒP(X, Y)ï¼Œéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§çš„è¡¡é‡ï¼š<br />
\(H(Y|X) = -\sum_{x,y}P(x, y)\log P(y|x)\)</p>

<p>äº’ä¿¡æ¯ (Mutual Information) è¡¨ç¤ºä¸¤ä¸ªä¿¡æ¯ç›¸äº¤çš„éƒ¨åˆ†ï¼š<br />
\(I(X;Y) = H(X)+H(Y) - H(X,Y)\)</p>

<p>ä¿¡æ¯å˜å·®(Variation of Information) è¡¨ç¤ºä¸¤ä¸ªäº‹ä»¶çš„ä¿¡æ¯ä¸æƒ³äº¤çš„éƒ¨åˆ†ï¼š<br />
\(VI(X;Y) = H(X)+H(Y) - 2I(X;Y)\)</p>

<pre><code class="language-python">p = np.linspace(1e-6,1-1e-6,1000) # ç”Ÿæˆæ¦‚ç‡
entropy = -p*np.log(p)-(1-p)*np.log(1-p) # é¦™å†œç†µ (Shannon Entropy)
plt.figure(figsize=(4,4))
plt.plot(p,entropy)
plt.title('Shannon Entropy')

</code></pre>

<pre><code>Text(0.5, 1.0, 'Shannon Entropy')
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_28_1.png" alt="png" /></p>

<p>ä¿¡æ¯å¢ç›Šï¼ˆKullbackâ€“Leibler divergenceï¼‰åˆç§°information divergenceï¼Œinformation gainï¼Œrelative entropy æˆ–è€…KLICã€‚
åœ¨æ¦‚ç‡è®ºå’Œä¿¡æ¯è®ºä¸­ï¼Œä¿¡æ¯å¢ç›Šæ˜¯éå¯¹ç§°çš„ï¼Œç”¨ä»¥åº¦é‡ä¸¤ç§æ¦‚ç‡åˆ†å¸ƒPå’ŒQçš„å·®å¼‚ã€‚<br />
\(D_{KL}(P||Q) = \sum_{x}P(x)\log \frac{P(x)}{Q(x)}\)</p>

<p>äº¤å‰ç†µ (Cross Entropy) å‡è®¾Pæ˜¯çœŸå®çš„åˆ†å¸ƒï¼ŒQæ˜¯é¢„æµ‹çš„åˆ†å¸ƒï¼Œé‚£ä¹ˆäº¤å‰ç†µå°±æ˜¯çœŸå®åˆ†å¸ƒå’Œé¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼š<br />
\(H(P, Q) = H(P) + D_{KL}(P||Q) = -\sum_{x}P(x)\log Q(x)\)</p>

<pre><code class="language-python"># D_{KL}(P||Q)  ä¸D_{KL}(Q||P)æ¯”è¾ƒ
x  = np.linspace(1,8,500)
y1 = norm.pdf(x,loc =3,scale = 0.5) # loc=3,scale=0.5è¡¨ç¤ºå‡å€¼ä¸º3ï¼Œæ ‡å‡†å·®ä¸º0.5
y2 = norm.pdf(x,loc =6,scale = 0.5) 
p = y1+y2 #æ„é€ p(x)
KL_pq ,KL_qp = [],[]
q_list = []
for mu in np.linspace(0,10,50):
    for sigma in np.linspace(0.1,1,50): # ç”Ÿæˆ50*50ä¸ªq(x),å¯»æ‰¾æœ€ä¼˜q(x)
        q = norm.pdf(x,loc =mu,scale = sigma) # æ„é€ q(x)
        q_list.append(q)
        KL_pq.append(np.sum(p*np.log(p/q))) # D_{KL}(P||Q)
        KL_qp.append(np.sum(q*np.log(q/p))) # D_{KL}(Q||P)
# min
min_KL_pq = np.argmin(KL_pq) # æœ€å°å€¼çš„ç´¢å¼•
min_KL_qp = np.argmin(KL_qp)

fig,axes = plt.subplots(1,2,figsize=(12,4))
axes[0].set_ylim(0,0.8)
axes[0].plot(x,p/2,'b',label='p(x)')
axes[0].plot(x,q_list[min_KL_pq],'r',label='$q^*(x)$')
axes[0].set_title('min D_{KL}(P||Q) ')
axes[0].set_xlabel('x')
axes[0].set_ylabel('p(x)')

axes[1].set_ylim(0,0.8)
axes[1].plot(x,p/2,'b',label='p(x)')
axes[1].plot(x,q_list[min_KL_qp],'r',label='$q^*(x)$')
axes[1].set_title('min D_{KL}(Q||P)')
axes[1].set_xlabel('x')
axes[1].set_ylabel('p(x)')
</code></pre>

<pre><code>&lt;ipython-input-60-0ceffff0da69&gt;:12: RuntimeWarning: divide by zero encountered in true_divide
  KL_pq.append(np.sum(p*np.log(p/q))) # D_{KL}(P||Q)
&lt;ipython-input-60-0ceffff0da69&gt;:12: RuntimeWarning: overflow encountered in true_divide
  KL_pq.append(np.sum(p*np.log(p/q))) # D_{KL}(P||Q)
&lt;ipython-input-60-0ceffff0da69&gt;:13: RuntimeWarning: divide by zero encountered in log
  KL_qp.append(np.sum(q*np.log(q/p))) # D_{KL}(Q||P)
&lt;ipython-input-60-0ceffff0da69&gt;:13: RuntimeWarning: invalid value encountered in multiply
  KL_qp.append(np.sum(q*np.log(q/p))) # D_{KL}(Q||P)





Text(0, 0.5, 'p(x)')
</code></pre>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/output_30_2.png" alt="png" /></p>

<p>ä¿¡æ¯å¢ç›Š (Information Gain) æ˜¯æŒ‡åœ¨çŸ¥é“ç‰¹å¾Xçš„æ¡ä»¶ä¸‹ï¼Œç‰¹å¾Yçš„ä¸ç¡®å®šæ€§å‡å°‘çš„ç¨‹åº¦ï¼š<br />
\(IG(X;Y) = H(Y) - H(Y|X)\)</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
from math import log
 
def create_data():
    datasets = [['é’å¹´', 'å¦', 'å¦', 'ä¸€èˆ¬', 'å¦'],
                ['é’å¹´', 'å¦', 'å¦', 'å¥½', 'å¦'],
                ['é’å¹´', 'æ˜¯', 'å¦', 'å¥½', 'æ˜¯'],
                ['é’å¹´', 'æ˜¯', 'æ˜¯', 'ä¸€èˆ¬', 'æ˜¯'],
                ['é’å¹´', 'å¦', 'å¦', 'ä¸€èˆ¬', 'å¦'],
                ['ä¸­å¹´', 'å¦', 'å¦', 'ä¸€èˆ¬', 'å¦'],
                ['ä¸­å¹´', 'å¦', 'å¦', 'å¥½', 'å¦'],
                ['ä¸­å¹´', 'æ˜¯', 'æ˜¯', 'å¥½', 'æ˜¯'],
                ['ä¸­å¹´', 'å¦', 'æ˜¯', 'éå¸¸å¥½', 'æ˜¯'],
                ['ä¸­å¹´', 'å¦', 'æ˜¯', 'éå¸¸å¥½', 'æ˜¯'],
                ['è€å¹´', 'å¦', 'æ˜¯', 'éå¸¸å¥½', 'æ˜¯'],
                ['è€å¹´', 'å¦', 'æ˜¯', 'å¥½', 'æ˜¯'],
                ['è€å¹´', 'æ˜¯', 'å¦', 'å¥½', 'æ˜¯'],
                ['è€å¹´', 'æ˜¯', 'å¦', 'éå¸¸å¥½', 'æ˜¯'],
                ['è€å¹´', 'å¦', 'å¦', 'ä¸€èˆ¬', 'å¦'],
                ]
    labels = [u'å¹´é¾„', u'æœ‰å·¥ä½œ', u'æœ‰è‡ªå·±çš„æˆ¿å­', u'ä¿¡è´·æƒ…å†µ', u'ç±»åˆ«']
    # è¿”å›æ•°æ®é›†å’Œæ¯ä¸ªç»´åº¦çš„åç§°
    return datasets, labels
 
 
# ç†µ
def calc_ent(datasets):
    data_length = len(datasets)
    label_count = {}
    for i in range(data_length):
        label = datasets[i][-1]
        if label not in label_count:
            label_count[label] = 0
        label_count[label] += 1
    ent = -sum([(p / data_length) * log(p / data_length, 2) for p in label_count.values()])
    return ent
 
 
# ç»éªŒæ¡ä»¶ç†µ
def cond_ent(datasets, axis=0):
    data_length = len(datasets)
    feature_sets = {}
    for i in range(data_length):
        feature = datasets[i][axis]
        if feature not in feature_sets:
            feature_sets[feature] = []
        feature_sets[feature].append(datasets[i])
    cond_ent = sum([(len(p) / data_length) * calc_ent(p) for p in feature_sets.values()])
    return cond_ent
 
 
# ä¿¡æ¯å¢ç›Š
def info_gain(ent, cond_ent):
    return ent - cond_ent
 
 
def info_gain_train(datasets):
    count = len(datasets[0]) - 1
    ent = calc_ent(datasets)
    best_feature = []
    for c in range(count):
        c_info_gain = info_gain(ent, cond_ent(datasets, axis=c))
        best_feature.append((c, c_info_gain))
        print('ç‰¹å¾({}) - info_gain - {:.3f}'.format(labels[c], c_info_gain))
    # æ¯”è¾ƒå¤§å°
    best_ = max(best_feature, key=lambda x: x[-1])
    return 'ç‰¹å¾({})çš„ä¿¡æ¯å¢ç›Šæœ€å¤§ï¼Œé€‰æ‹©ä¸ºæ ¹èŠ‚ç‚¹ç‰¹å¾'.format(labels[best_[0]])
 
datasets, labels = create_data()
train_data = pd.DataFrame(datasets, columns=labels)
print(train_data)
print('ç‰¹å¾ä¿¡æ¯å¢ç›Šä¸ºï¼š', info_gain_train(np.array(datasets)))
</code></pre>

<pre><code>    å¹´é¾„ æœ‰å·¥ä½œ æœ‰è‡ªå·±çš„æˆ¿å­ ä¿¡è´·æƒ…å†µ ç±»åˆ«
0   é’å¹´   å¦      å¦   ä¸€èˆ¬  å¦
1   é’å¹´   å¦      å¦    å¥½  å¦
2   é’å¹´   æ˜¯      å¦    å¥½  æ˜¯
3   é’å¹´   æ˜¯      æ˜¯   ä¸€èˆ¬  æ˜¯
4   é’å¹´   å¦      å¦   ä¸€èˆ¬  å¦
5   ä¸­å¹´   å¦      å¦   ä¸€èˆ¬  å¦
6   ä¸­å¹´   å¦      å¦    å¥½  å¦
7   ä¸­å¹´   æ˜¯      æ˜¯    å¥½  æ˜¯
8   ä¸­å¹´   å¦      æ˜¯  éå¸¸å¥½  æ˜¯
9   ä¸­å¹´   å¦      æ˜¯  éå¸¸å¥½  æ˜¯
10  è€å¹´   å¦      æ˜¯  éå¸¸å¥½  æ˜¯
11  è€å¹´   å¦      æ˜¯    å¥½  æ˜¯
12  è€å¹´   æ˜¯      å¦    å¥½  æ˜¯
13  è€å¹´   æ˜¯      å¦  éå¸¸å¥½  æ˜¯
14  è€å¹´   å¦      å¦   ä¸€èˆ¬  å¦
ç‰¹å¾(å¹´é¾„) - info_gain - 0.083
ç‰¹å¾(æœ‰å·¥ä½œ) - info_gain - 0.324
ç‰¹å¾(æœ‰è‡ªå·±çš„æˆ¿å­) - info_gain - 0.420
ç‰¹å¾(ä¿¡è´·æƒ…å†µ) - info_gain - 0.363
ç‰¹å¾ä¿¡æ¯å¢ç›Šä¸ºï¼š ç‰¹å¾(æœ‰è‡ªå·±çš„æˆ¿å­)çš„ä¿¡æ¯å¢ç›Šæœ€å¤§ï¼Œé€‰æ‹©ä¸ºæ ¹èŠ‚ç‚¹ç‰¹å¾
</code></pre>

<h2 id="23-å›¾æ¨¡å‹">2.3 å›¾æ¨¡å‹</h2>
<p>å›¾æ¨¡å‹ï¼ˆGraphical Model)æ˜¯ä¸€ç§è¡¨ç¤ºæ¦‚ç‡åˆ†å¸ƒçš„æ–¹æ³•ï¼Œå®ƒå°†æ¦‚ç‡åˆ†å¸ƒè¡¨ç¤ºä¸ºä¸€ä¸ªå›¾ï¼Œå›¾ä¸­çš„èŠ‚ç‚¹è¡¨ç¤ºéšæœºå˜é‡ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„è¾¹è¡¨ç¤ºéšæœºå˜é‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚</p>

<h3 id="231-æœ‰å‘å›¾æ¨¡å‹-directed-graphical-model">2.3.1 æœ‰å‘å›¾æ¨¡å‹ directed graphical model</h3>

<p>æœ‰å‘å›¾æ¨¡å‹æ˜¯æŒ‡å›¾ä¸­çš„è¾¹æ˜¯æœ‰å‘çš„ï¼Œå³è¾¹çš„æ–¹å‘è¡¨ç¤ºäº†éšæœºå˜é‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æœ‰å‘å›¾æ¨¡å‹å¯ä»¥è¡¨ç¤ºæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ(CPD)ï¼Œä¹Ÿå¯ä»¥è¡¨ç¤ºé©¬å°”ç§‘å¤«éšæœºåœºã€‚</p>

<p>æœ‰å‘å›¾çš„ä»£è¡¨æ˜¯è´å¶æ–¯ç½‘ã€‚
è´å¶æ–¯â½¹ä¸æœ´ç´ è´å¶æ–¯æ¨¡å‹å»ºâ½´åœ¨ç›¸åŒçš„ç›´è§‚å‡è®¾ä¸Šï¼šé€šè¿‡åˆ©ç”¨åˆ†å¸ƒçš„æ¡ä»¶ç‹¬ç«‹æ€§æ¥è·å¾—ç´§å‡‘è€Œè‡ªç„¶çš„è¡¨ç¤ºã€‚è´å¶æ–¯â½¹æ ¸â¼¼æ˜¯â¼€ä¸ªæœ‰å‘â½†ç¯å›¾(DAG)ï¼Œå…¶èŠ‚ç‚¹ä¸ºè®ºåŸŸä¸­çš„éšæœºå˜é‡ï¼ŒèŠ‚ç‚¹é—´çš„æœ‰å‘ç®­å¤´è¡¨â½°è¿™ä¸¤ä¸ªèŠ‚ç‚¹çš„ä¾èµ–å…³ç³»ã€‚</p>

<p><img src="/images/posts/2022-12-16-æœºå™¨å­¦ä¹ å…¥é—¨2_æ¦‚ç‡è®º/æœ‰å‘å›¾ç¤ºä¾‹.jpg" alt="æœ‰å‘å›¾ç¤ºä¾‹" /></p>

<p>æœ‰å‘â½†ç¯å›¾å¯ä»¥ç”±å¦‚ä¸‹ 3 ç§å…ƒç»“æ„æ„æˆï¼š</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>åŒâ½—ç»“æ„ã€‚ä¾‹å¦‚åœ¨ä¸Šå›¾ä¸­ï¼Œè‹¥ä¸è€ƒè™‘èŠ‚ç‚¹ a ï¼Œåˆ™ c å’Œ d æœ‰åŒâ¼€â½—èŠ‚ç‚¹ bï¼Œäºæ˜¯ $P(b, c, d) = P(b)P(c</td>
          <td>b)P(d</td>
          <td>b)$ã€‚</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>V å‹ç»“æ„ã€‚ä¾‹å¦‚åœ¨ä¸Šå›¾ä¸­ï¼Œè‹¥ä¸è€ƒè™‘èŠ‚ç‚¹ a åˆ° b çš„ä¾èµ–å…³ç³»ï¼Œåˆ™ $P(a, b, c) = P(a)P(b)P(c</td>
          <td>a, b)$ã€‚</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>é¡ºåºç»“æ„ã€‚ä¾‹å¦‚åœ¨ä¸Šå›¾ä¸­ï¼Œè‹¥ä»…è€ƒè™‘èŠ‚ç‚¹ aã€bã€dï¼Œåˆ™æœ‰ $P(a, b, d) = P(a)P(b</td>
          <td>a)P(d</td>
          <td>b)$ã€‚</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯¹ç®€åŒ–çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ (å¦‚ $P(c</td>
          <td>a, b)$ )è·å–æ¡ä»¶æ¦‚ç‡è¡¨ã€‚åŒæ—¶ï¼Œä¹Ÿå¯ä»¥æ±‚å¾—è”åˆæ¦‚ç‡åˆ†å¸ƒ $P(a, b, c, d, e) = P(a)P(b</td>
          <td>a)P(c</td>
          <td>a, b)P(d</td>
          <td>b)P(e</td>
          <td>c)$ã€‚</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>è´å¶æ–¯ç½‘çš„ç‹¬ç«‹æ€§</p>
<ul>
  <li>å±€éƒ¨ç‹¬ç«‹æ€§</li>
  <li>å…¨å±€ç‹¬ç«‹æ€§
    <ul>
      <li>tail-to-tail</li>
      <li>head-to-tail</li>
      <li>head-to-head
è€ƒè™‘å¤æ‚çš„æœ‰å‘æ— ç¯å›¾ï¼Œå¦‚æœA,B,Cæ˜¯ä¸‰ä¸ªé›†åˆï¼Œå¯ä»¥æ˜¯å•ç‹¬çš„èŠ‚ç‚¹æˆ–è€…èŠ‚ç‚¹çš„é›†åˆã€‚ä¸ºäº†åˆ¤æ–­Aå’ŒBæ˜¯å¦æ—¶Cæ¡ä»¶ç‹¬ç«‹çš„ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘Aå’ŒBä¹‹é—´çš„æ‰€æœ‰è·¯å¾„ã€‚å¯¹äºå…¶ä¸­çš„ä¸€æ¡è·¯å¾„ï¼Œå¦‚æœæ»¡è¶³ä»¥ä¸‹ä¸¤ç‚¹çš„ä»»æ„ä¸€ç‚¹ï¼Œé‚£ä¹ˆå°±è¯´è¿™æ¡è·¯å¾„æ˜¯é˜»å¡ï¼ˆblockedï¼‰çš„ï¼š
        <ol>
          <li>è·¯å¾„ä¸­å­˜åœ¨æŸä¸ªèŠ‚ç‚¹ X æ˜¯ head-to-tail æˆ–è€… tail-to-tail èŠ‚ç‚¹ï¼Œå¹¶ä¸” X æ˜¯åŒ…å«åœ¨ C ä¸­çš„ï¼›</li>
          <li>è·¯å¾„ä¸­å­˜åœ¨æŸä¸ªèŠ‚ç‚¹ X æ˜¯ head-to-head èŠ‚ç‚¹ï¼Œå¹¶ä¸” X æˆ– X çš„â¼‰â¼¦æ˜¯ä¸åŒ…å«åœ¨ C ä¸­çš„ã€‚</li>
        </ol>

        <p>å¦‚æœ A,B é—´æ‰€æœ‰çš„è·¯å¾„éƒ½æ˜¯é˜»å¡çš„ï¼Œé‚£ä¹ˆ A,B å°±æ˜¯å…³äº C æ¡ä»¶ç‹¬â½´çš„ï¼›å¦åˆ™ A,B ä¸æ˜¯å…³äº C æ¡ä»¶ç‹¬â½´çš„ã€‚</p>
      </li>
    </ul>
  </li>
</ul>

<pre><code class="language-python">import networkx as nx
from pgmpy.models import BayesianModel
from pgmpy.factors.discrete import TabularCPD
import matplotlib.pyplot as plt
%matplotlib inline

# å»ºç«‹ä¸€ä¸ªç®€å•è´å¶æ–¯æ¨¡å‹æ¡†æ¶
model = BayesianModel([('a', 'b'), ('a', 'c'), ('b', 'c'), ('b', 'd'), ('c', 'e')])
# æœ€é¡¶å±‚çš„çˆ¶èŠ‚ç‚¹çš„æ¦‚ç‡åˆ†å¸ƒè¡¨
cpd_a = TabularCPD(variable='a', variable_card=2, values=[[0.6, 0.4]]) # a: (0,1)

# å…¶å®ƒå„èŠ‚ç‚¹çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒè¡¨ï¼ˆè¡Œå¯¹åº”å½“å‰èŠ‚ç‚¹ç´¢å¼•ï¼Œåˆ—å¯¹åº”çˆ¶èŠ‚ç‚¹ç´¢å¼•ï¼‰
cpd_b = TabularCPD(variable='b', variable_card=2, # b: (0,1)
values=[[0.75, 0.1],
[0.25, 0.9]],
evidence=['a'],
evidence_card=[2])
cpd_c = TabularCPD(variable='c', variable_card=3, # c: (0,1,2)
values=[[0.3, 0.05, 0.9, 0.5],
[0.4, 0.25, 0.08, 0.3],
[0.3, 0.7, 0.02, 0.2]],
evidence=['a', 'b'],
evidence_card=[2, 2])
cpd_d = TabularCPD(variable='d', variable_card=2, # d: (0,1)
values=[[0.95, 0.2],
[0.05, 0.8]],
evidence=['b'],
evidence_card=[2])
cpd_e = TabularCPD(variable='e', variable_card=2, # e: (0,1)
values=[[0.1, 0.4, 0.99],
[0.9, 0.6, 0.01]],
evidence=['c'],
evidence_card=[3])

# å°†å„èŠ‚ç‚¹çš„æ¦‚ç‡åˆ†å¸ƒè¡¨åŠ å…¥ç½‘ç»œ
model.add_cpds(cpd_a, cpd_b, cpd_c, cpd_d, cpd_e)
# éªŒè¯æ¨¡å‹æ•°æ®çš„æ­£ç¡®æ€§
print(u"éªŒè¯æ¨¡å‹æ•°æ®çš„æ­£ç¡®æ€§:",model.check_model())
# ç»˜åˆ¶è´å¶æ–¯å›¾ (èŠ‚ç‚¹ + ä¾èµ–å…³ç³»)
nx.draw(model, with_labels=True, node_size=1000, font_weight='bold', node_color='y', \
pos={"e":[4,3],"c":[4,5],"d":[8,5],"a":[2,7],"b":[6,7]})
plt.text(2,7,model.get_cpds("a"), fontsize=10, color='b')
plt.text(5,6,model.get_cpds("b"), fontsize=10, color='b')
plt.text(1,4,model.get_cpds("c"), fontsize=10, color='b')
plt.text(4.2,2,model.get_cpds("e"), fontsize=10, color='b')
plt.text(7,3.4,model.get_cpds("d"), fontsize=10, color='b')

plt.show()
</code></pre>

<pre><code>---------------------------------------------------------------------------

ModuleNotFoundError                       Traceback (most recent call last)

&lt;ipython-input-1-96c0e1595c11&gt; in &lt;module&gt;
      1 import networkx as nx
----&gt; 2 from pgmpy.models import BayesianModel
      3 from pgmpy.factors.discrete import TabularCPD
      4 import matplotlib.pyplot as plt
      5 get_ipython().run_line_magic('matplotlib', 'inline')


ModuleNotFoundError: No module named 'pgmpy'
</code></pre>

<h3 id="232-æ— å‘å›¾æ¨¡å‹-undirected-model">2.3.2 æ— å‘å›¾æ¨¡å‹ (Undirected Model)</h3>
<p>é©¬å°”å¯å¤«ç½‘èŠ‚ç‚¹é—´çš„ä¾èµ–å…³ç³»æ˜¯æ— å‘çš„ï¼ˆç›¸äº’å¹³ç­‰çš„å…³ç³»ï¼‰ï¼Œâ½†æ³•â½¤æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæ¥è¡¨â½°ï¼Œä¸ºæ­¤ä¸ºå¼•â¼Šæå¤§å›¢æ¦‚å¿µï¼Œè¿›â½½ä¸ºæ¯ä¸ªæâ¼¤å›¢å¼•â¼Šâ¼€ä¸ªåŠ¿å‡½æ•°ä½œä¸ºå› â¼¦ï¼Œç„¶åå°†è”åˆæ¦‚ç‡åˆ†å¸ƒè¡¨â½°æˆè¿™äº›å› â¼¦çš„ä¹˜ç§¯å†å½’â¼€åŒ–ï¼Œå½’â¼€åŒ–å¸¸æ•°è¢«ç§°ä½œé…åˆ†å‡½æ•°ã€‚</p>

:ET